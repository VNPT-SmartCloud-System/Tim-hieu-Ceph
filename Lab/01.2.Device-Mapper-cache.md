# Device Mapper cache - dm-cache
dm-cache (Device Mapper cache) là 1 device mapper target để cải thiện performance của 1 a block device (vd: 1 spindle device ) HDD bằng cách tự động migrating data của nó sang thiết bị nhanh và nhỏ hơn (SSD). NVMe based PCIe SSD sẽ hoạt động như thiết bị cache device. dm-cache có thể được cấu hình sử dụng user space tool set LVM2( logical volume manager tools).

Virtual cache được tạo bằng dm-cache tạo từ 3 thiết bị vật lý. Origin device là HDD. Cache device sử dụng để lưu user data block và meta device lưu meta data như block placement, dirty flags và other internal data. Trong trường hợp này sử dụng cùng thiết bị Cache device cho cả data blocks và metadata.

<img src=https://i.imgur.com/JvPQEor.png>

**Hardware Setup:**
- Origin Device HDD: /dev/sda1
- Cache Device [NVMe based PCIe SSD] (Block/Meta): /dev/nvme0n1p1

**Software Setup:**
- Operating system : Rhel 7
- Tools : LVM2 user space tools cho cấu hình dm-cache

## 1. Cấu hình

### 1.1 Tạo phân vùng 10GB trên HDD làm Origin Device
```sh
parted -a optimal /dev/vdb mkpart primary 1 10G
```
This creates /dev/sda1
### 1.2. Tạo phân vùng 10GB trên NVMe PCIe SSD disk làm Cache Device
```sh
parted -a optimal /dev/vdc mkpart primary 1 10G
```
### 1.3. Tạo Volume group (cache) với Origin và Cache device
```
vgcreate cache /dev/vdb1 /dev/vdc1
```
Kiểm tra Volume Group  `cache` bằng lệnh `vgdisplay`
```sh
[root@ceph02 ~]# vgdisplay
  --- Volume group ---
  VG Name               centos_ceph02
  System ID
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  3
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                2
  Open LV               2
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               <29.00 GiB
  PE Size               4.00 MiB
  Total PE              7423
  Alloc PE / Size       7422 / 28.99 GiB
  Free  PE / Size       1 / 4.00 MiB
  VG UUID               3VvY9b-ep2n-8ncc-alpU-sEWX-dcLt-vFbg61

  --- Volume group ---
  VG Name               cache
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               19.99 GiB
  PE Size               4.00 MiB
  Total PE              5118
  Alloc PE / Size       0 / 0
  Free  PE / Size       5118 / 19.99 GiB
  VG UUID               Ylc3s5-2HXI-UecF-CAAk-mYbb-WUmM-cxCmC4
  ```
### 1.4. Tạo Logical Volume Origin Device (origin_device)
```sh
lvcreate -l 90%FREE -n origin_device cache /dev/vdb1
```
### 1.5. Tạo Logical Volume cache meta data (cache_meta)
```sh
lvcreate -l 5%FREE -n cache_meta cache /dev/vdc1
```
### 1.6. Tạo Logical Volume cache block data (cache_block)
```sh
lvcreate -l 80%FREE -n cache_block cache /dev/vdc1
```
### 1.7. Tạo 1 Logical Volume cache pool bằng cách két hợp Logical Volumes (Bước 5 và 6) `cache block` và `cache meta data`.
```sh
lvconvert --type cache-pool --poolmetadata cache/cache_meta cache/cache_block
```
Lưu ý: Việc này sẽ tạo 1 cache pool cùng tên `cache_block` của Logical Volume `cache block data` ở bước 6

Kiểm tra Volume Group `cache` bằng lệnh `lvdisplay`
```sh
$ lvdisplay
  --- Logical volume ---
  LV Path                /dev/cache/cache_block
  LV Name                cache_block
  VG Name                cache
  LV UUID                H531Yq-DMhw-QXnt-Rinv-Qulb-f9dG-SzdOAf
  LV Write Access        read/write
  LV Creation host, time ceph02, 2020-04-13 14:28:55 +0700
  LV Pool metadata       cache_block_cmeta
  LV Pool data           cache_block_cdata
  LV Status              NOT available
  LV Size                <7.60 GiB
  Current LE             1945
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
```
### 1.8. Tạo 1 Logical Volume cache bằng cách linking Logical Volume cache pool (Bước 7) và Logical Volume origin_device (Bước 4)
```sh
lvconvert --type cache --cachepool cache/cache_block cache/origin_device
```
Việc này tạo ra virtual cache device (dm-cache) orign_device `dm2` như dưới đây
```sh
$ ls -l /dev/cache/origin_device
lrwxrwxrwx 1 root root 7 Apr 13 14:39 /dev/cache/origin_device -> ../dm-2
```
### 1.9.Format Virtual cache device và sử dụng
```sh
$ mkfs.ext4 /dev/cache/origin_device
```
## 2. Cài đặt công cụ benchmark 
### 2.1 Cài đặt Fio
Mặc định Fio không có tròn repository CentOS. Cần cài đặt EPEL repository. Sau đó cài Fio
```sh
yum install epel-release -y
yum install fio -y
```
### 2.2. Test với Fio

- Mount file
```sh
mount /dev/cache/origin_device /mnt
```

- **Random Write Test**

Ghi 1 file tổng 2GB (4 jobs x 512 MB = 2GB) chạy trong 4 processes cùng 1 thời điểm:
```sh
$ cd /mnt
$ fio --name=randwrite --ioengine=libaio --iodepth=1 --rw=randwrite --bs=4k --direct=0 --size=512M --numjobs=4 --runtime=240 --group_reporting
```
Kết quả tương tự nhu sau:
```sh
randwrite: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1
fio-2.1.3
Starting 4 processes
randwrite: Laying out IO file(s) (1 file(s) / 512MB)
randwrite: Laying out IO file(s) (1 file(s) / 512MB)
randwrite: Laying out IO file(s) (1 file(s) / 512MB)
randwrite: Laying out IO file(s) (1 file(s) / 512MB)
Jobs: 1 (f=1): [__w_] [-.-% done] [0KB/27032KB/0KB /s] [0/6758/0 iops] [eta 00m:00s] 
randwrite: (groupid=0, jobs=4): err= 0: pid=28810: Tue Jul 25 20:28:20 2017
  write: io=2048.0MB, bw=297722KB/s, iops=74430, runt=  7044msec
    slat (usec): min=9, max=47469, avg=38.13, stdev=488.37
    clat (usec): min=1, max=10595, avg= 2.26, stdev=30.99
     lat (usec): min=11, max=47478, avg=40.74, stdev=489.66
    clat percentiles (usec):
     |  1.00th=[    1],  5.00th=[    2], 10.00th=[    2], 20.00th=[    2],
     | 30.00th=[    2], 40.00th=[    2], 50.00th=[    2], 60.00th=[    2],
     | 70.00th=[    2], 80.00th=[    2], 90.00th=[    2], 95.00th=[    2],
     | 99.00th=[    3], 99.50th=[    4], 99.90th=[   16], 99.95th=[   61],
     | 99.99th=[  454]
    bw (KB  /s): min=10626, max=244432, per=32.42%, avg=96530.07, stdev=85594.86
    lat (usec) : 2=4.28%, 4=95.15%, 10=0.44%, 20=0.03%, 50=0.03%
    lat (usec) : 100=0.04%, 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%
    lat (msec) : 2=0.01%, 4=0.01%, 10=0.01%, 20=0.01%
  cpu          : usr=11.91%, sys=27.58%, ctx=4646, majf=0, minf=104
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=524288/d=0, short=r=0/w=0/d=0

Run status group 0 (all jobs):
  WRITE: io=2048.0MB, aggrb=297721KB/s, minb=297721KB/s, maxb=297721KB/s, mint=7044msec, maxt=7044msec

Disk stats (read/write):
  sda: ios=129/2495, merge=0/1599, ticks=11520/91476, in_queue=143340, util=78.35%
```
- **Random Read Test**

Đọc file 2GB, chạy 4 tiến trình cùng 1 luc.
```sh
$ fio --name=randread --ioengine=libaio --iodepth=16 --rw=randread --bs=4k --direct=0 --size=512M --numjobs=4 --runtime=240 --group_reporting
```
Kết quả tương tự như sau:
```sh
randread: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=16
...
randread: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=16
fio-2.1.3
Starting 4 processes
randread: Laying out IO file(s) (1 file(s) / 512MB)
randread: Laying out IO file(s) (1 file(s) / 512MB)
randread: Laying out IO file(s) (1 file(s) / 512MB)
randread: Laying out IO file(s) (1 file(s) / 512MB)
Jobs: 4 (f=4): [rrrr] [100.0% done] [467KB/0KB/0KB /s] [116/0/0 iops] [eta 00m:00s]
randread: (groupid=0, jobs=4): err= 0: pid=14521: Tue Jul 25 22:01:24 2017
  read : io=117912KB, bw=503032B/s, iops=122, runt=240028msec
    slat (usec): min=93, max=1178.5K, avg=32554.59, stdev=45398.62
    clat (usec): min=12, max=2611.2K, avg=487669.64, stdev=288615.07
     lat (msec): min=49, max=2667, avg=520.23, stdev=303.69
    clat percentiles (msec):
     |  1.00th=[  167],  5.00th=[  217], 10.00th=[  247], 20.00th=[  289],
     | 30.00th=[  330], 40.00th=[  367], 50.00th=[  404], 60.00th=[  445],
     | 70.00th=[  502], 80.00th=[  603], 90.00th=[  898], 95.00th=[ 1057],
     | 99.00th=[ 1614], 99.50th=[ 1811], 99.90th=[ 2245], 99.95th=[ 2376],
     | 99.99th=[ 2573]
    bw (KB  /s): min=    4, max=  318, per=25.59%, avg=125.65, stdev=59.18
    lat (usec) : 20=0.01%
    lat (msec) : 50=0.01%, 100=0.02%, 250=10.45%, 500=59.10%, 750=15.73%
    lat (msec) : 1000=8.21%, 2000=6.23%, >=2000=0.25%
  cpu          : usr=0.05%, sys=0.17%, ctx=29816, majf=0, minf=165
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=99.8%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=29478/w=0/d=0, short=r=0/w=0/d=0

Run status group 0 (all jobs):
   READ: io=117912KB, aggrb=491KB/s, minb=491KB/s, maxb=491KB/s, mint=240028msec, maxt=240028msec

Disk stats (read/write):
  sda: ios=29552/2717, merge=0/2735, ticks=969368/97672, in_queue=1067872, util=100.00%
```
- **Read Write Performance Test**

Đo ngẫu nhiên read/write performance của USB Pen drive (/dev/sdc1):
Ghi 20MB data vào USB Pen drive và thực hiện đọc và ghi 4KB sử dunngj 3 lần đọc cho mỗi lần ghi (rwmixread=75).

```sh
$ fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=random_read_write.fio --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75
```
Kết quả tương tự như sau:
```sh
test: (g=0): rw=randrw, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64
fio-2.1.3
Starting 1 process
Jobs: 1 (f=1): [m] [98.0% done] [191KB/95KB/0KB /s] [47/23/0 iops] [eta 00m:03s]
test: (groupid=0, jobs=1): err= 0: pid=24800: Tue Jul 25 22:10:38 2017
  read : io=15512KB, bw=107237B/s, iops=26, runt=148123msec
  write: io=4968.0KB, bw=34344B/s, iops=8, runt=148123msec
  cpu          : usr=0.08%, sys=0.18%, ctx=5080, majf=0, minf=24
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.2%, 16=0.3%, 32=0.6%, >=64=98.8%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued    : total=r=3878/w=1242/d=0, short=r=0/w=0/d=0

Run status group 0 (all jobs):
   READ: io=15512KB, aggrb=104KB/s, minb=104KB/s, maxb=104KB/s, mint=148123msec, maxt=148123msec
  WRITE: io=4968KB, aggrb=33KB/s, minb=33KB/s, maxb=33KB/s, mint=148123msec, maxt=148123msec

Disk stats (read/write):
  sdc: ios=3506/1203, merge=372/28, ticks=7386380/1400600, in_queue=8796700, util=100.00%
```
## Tài liệu tham khảo
- https://www.dell.com/support/article/en-vn/sln312372/faster-block-device-performance-with-nvme-pcie-ssd-based-dm-cache-on-rhel-7?lang=en
- https://linoxide.com/linux-how-to/measure-disk-performance-fio/
