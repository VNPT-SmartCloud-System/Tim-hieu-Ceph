# Install Ceph Octopus using cephadm

## 1. Install NTP
```sh
yum install chrony -y
timedatectl set-timezone Asia/Ho_Chi_Minh

sed -i 's/server 0.centos.pool.ntp.org iburst/ \
server 0.asia.pool.ntp.org iburst \
server 1.asia.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 1.centos.pool.ntp.org iburst/#server 1.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 2.centos.pool.ntp.org iburst/#server 2.centos.pool.ntp.org iburst/g' /etc/chrony.conf
sed -i 's/server 3.centos.pool.ntp.org iburst/#server 3.centos.pool.ntp.org iburst/g' /etc/chrony.conf
```
Start NTP
```sh
systemctl enable chronyd.service
systemctl start chronyd.service
```
## 2. Install docker
```sh
yum install docker -y
```
Start and enable docker
```sh
systemctl start docker
systemctl enable docker
```
## 3. Install Cephadm
Install cephadm
```sh
./cephadm add-repo --release octopus
./cephadm install
```
Hoặc
```sh
yum install -y cephadm
```
### 3.1 Bootstrap a new Cluster
```sh
mkdir -p /etc/ceph
cephadm bootstrap --mon-ip 172.16.68.61
```
### 3.2 ENABLE CEPH CLI
Cephadm không yêu cầy bất kỳ `Ceph packages` nào để cài đặt trên host. Tuy nhiên để dễ dàng sử dụng ceph command. Có nhiều cách để thực hiện điều này:

- `cephadm` chạy 1 bash shell trong 1 container với tất cả các Ceph packages được cài đặt. Mặc định nếu file `configuration` và `keyring` được tìm thấy trong `/etc/ceph` trên host, they are passed into the container environment so that the shell is fully functional:
```sh
cephadm shell
```
- Có thể cài đặt `ceph-common` package chưa tất cả `ceph commands` bao gồm `ceph`, `rbd`, `mount.ceph` (cho mounting CephFS file systems)...:
```sh
cephadm add-repo --release octopus
cephadm install ceph-common
```
Confirm that the ceph command is accessible with:
```sh
ceph -v
```
Confirm that the ceph command can connect to the cluster and also its status with:
```sh
ceph status
```
## 4. Add Host to Cluster
Để add 1 host vào Cluster thực hiện 2 bước sau:

Copy SSH-key đến Node mới:
```sh
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph02
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph03
```
Add Node mới vào Cluster:
```sh
ceph orch host add ceph02
ceph orch host add ceph03
```
## 5.
## 6. Ddeploy OSD
List danh sách disk trên tất cả các Node trong Cluster:
```sh
ceph orch device ls
```
Tạo OSD mới bằng 1 số cách sau:
- Cho tất cả các disk available 
```sh
ceph orch apply osd --all-available-devices
```
- Chỉ định disk làm OSD trên Node:
```sh
ceph orch daemon add osd ceph01:/dev/sdb
```
- Use OSD Service Specification to describe device(s) to consume based on their properties, such device type (SSD or HDD), device model names, size, or the hosts on which the devices exist:
```sh
ceph orch apply osd -i spec.yml
```




## Tài liệu tham khảo
- https://docs.ceph.com/docs/master/cephadm/install/
